{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MDUUcFa7O2u"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-ai-edge/model-explorer/blob/main/example_colabs/custom_data_overlay_demo.ipynb)\n",
        "\n",
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "me12eMftdu4H"
      },
      "outputs": [],
      "source": [
        "# Install tflite & model-explorer.\n",
        "!pip install tflite\n",
        "!pip install --no-deps ai-edge-model-explorer ai-edge-model-explorer-adapter\n",
        "\n",
        "# Install kagglehub (will be used in the next step to download a model)\n",
        "!pip install kagglehub --no-deps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im5BQ2Qz7na_"
      },
      "source": [
        "# Download MobileNet v3 from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaSxh0rQf4Fj"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# This demo uses MobileNet v3, but you can use other models as well\n",
        "path = kagglehub.model_download(\"google/mobilenet-v3/tfLite/large-075-224-classification\")\n",
        "model_path = f\"{path}/1.tflite\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWm4BE_I70hi"
      },
      "source": [
        "# Run the model with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cuAg62OvfTK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Generate random input data.\n",
        "for input_detail in input_details:\n",
        "  input_shape = input_detail['shape']\n",
        "  input_data = np.array(np.random.random_sample(input_shape), dtype=input_detail['dtype'])\n",
        "  interpreter.set_tensor(input_detail['index'], input_data)\n",
        "\n",
        "# Run the model on random input data.\n",
        "interpreter.invoke()\n",
        "\n",
        "# Examine the output data (optional)\n",
        "for output_detail in output_details:\n",
        "  print(f\"Output for {output_detail['name']}\")\n",
        "  output_data = interpreter.get_tensor(output_detail['index'])\n",
        "  print(output_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhwvzBUIO9SZ"
      },
      "source": [
        "# Prepare per-op benchmarking data for Model Explorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2FLsz9OsC6w_"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /tmp/data\n",
        "%cd /tmp/data\n",
        "\n",
        "# In this example, we're using profiling data from Android's Benchmarking tools\n",
        "# that has already been mapped (outside of this Colab) to the Model Explorer schema.\n",
        "\n",
        "# You can overlay per-op data of your choice by following the instructions at\n",
        "# https://github.com/google/model-explorer/wiki/2.-User-Guide#custom-node-data\n",
        "\n",
        "!wget -nc https://storage.googleapis.com/tfweb/model-explorer-demo/mv3-cpu-op-profile.json\n",
        "!wget -nc https://storage.googleapis.com/tfweb/model-explorer-demo/mv3-xnnpack-op-profile.json\n",
        "CPU_PROFILING_JSON_PATH=\"/tmp/data/mv3-cpu-op-profile.json\"\n",
        "XNNPACK_PROFILING_JSON_PATH=\"/tmp/data/mv3-xnnpack-op-profile.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCgFNFP4WAzE"
      },
      "source": [
        "# Visualize the model with per op latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SVyHrthM_hz"
      },
      "outputs": [],
      "source": [
        "import model_explorer\n",
        "\n",
        "config = model_explorer.config()\n",
        "(config\n",
        " .add_model_from_path(model_path)\n",
        " .add_node_data_from_path(CPU_PROFILING_JSON_PATH)\n",
        " .add_node_data_from_path(XNNPACK_PROFILING_JSON_PATH))\n",
        "\n",
        "model_explorer.visualize_from_config(config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
